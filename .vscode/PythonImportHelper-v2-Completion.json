[
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "bibtex",
        "importPath": "pybtex.database.input",
        "description": "pybtex.database.input",
        "isExtraImport": true,
        "detail": "pybtex.database.input",
        "documentation": {}
    },
    {
        "label": "pybtex.database.input.bibtex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pybtex.database.input.bibtex",
        "description": "pybtex.database.input.bibtex",
        "detail": "pybtex.database.input.bibtex",
        "documentation": {}
    },
    {
        "label": "strptime",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "html",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "html",
        "description": "html",
        "detail": "html",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "date",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "frontmatter",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "frontmatter",
        "description": "frontmatter",
        "detail": "frontmatter",
        "documentation": {}
    },
    {
        "label": "getorg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "getorg",
        "description": "getorg",
        "detail": "getorg",
        "documentation": {}
    },
    {
        "label": "Nominatim",
        "importPath": "geopy",
        "description": "geopy",
        "isExtraImport": true,
        "detail": "geopy",
        "documentation": {}
    },
    {
        "label": "GeocoderTimedOut",
        "importPath": "geopy.exc",
        "description": "geopy.exc",
        "isExtraImport": true,
        "detail": "geopy.exc",
        "documentation": {}
    },
    {
        "label": "html_escape",
        "kind": 2,
        "importPath": "markdown_generator.publications",
        "description": "markdown_generator.publications",
        "peekOfCode": "def html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\n# ## Creating the markdown files\n# \n# This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page. If you don't want something to appear (like the \"Recommended citation\")\n# In[5]:\nimport os\nfor row, item in publications.iterrows():\n    md_filename = str(item.pub_date) + \"-\" + item.url_slug + \".md\"",
        "detail": "markdown_generator.publications",
        "documentation": {}
    },
    {
        "label": "publications",
        "kind": 5,
        "importPath": "markdown_generator.publications",
        "description": "markdown_generator.publications",
        "peekOfCode": "publications = pd.read_csv(\"publications.tsv\", sep=\"\\t\", header=0)\npublications\n# ## Escape special characters\n# \n# YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely.\n# In[4]:\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"",
        "detail": "markdown_generator.publications",
        "documentation": {}
    },
    {
        "label": "html_escape_table",
        "kind": 5,
        "importPath": "markdown_generator.publications",
        "description": "markdown_generator.publications",
        "peekOfCode": "html_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\n# ## Creating the markdown files\n# ",
        "detail": "markdown_generator.publications",
        "documentation": {}
    },
    {
        "label": "html_escape",
        "kind": 2,
        "importPath": "markdown_generator.pubsFromBib",
        "description": "markdown_generator.pubsFromBib",
        "peekOfCode": "def html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\nfor pubsource in publist:\n    parser = bibtex.Parser()\n    bibdata = parser.parse_file(publist[pubsource][\"file\"])\n    #loop through the individual references in a given bibtex file\n    for bib_id in bibdata.entries:\n        #reset default date\n        pub_year = \"1900\"",
        "detail": "markdown_generator.pubsFromBib",
        "documentation": {}
    },
    {
        "label": "publist",
        "kind": 5,
        "importPath": "markdown_generator.pubsFromBib",
        "description": "markdown_generator.pubsFromBib",
        "peekOfCode": "publist = {\n    \"proceeding\": {\n        \"file\" : \"proceedings.bib\",\n        \"venuekey\": \"booktitle\",\n        \"venue-pretext\": \"In the proceedings of \",\n        \"collection\" : {\"name\":\"publications\",\n                        \"permalink\":\"/publication/\"}\n    },\n    \"journal\":{\n        \"file\": \"pubs.bib\",",
        "detail": "markdown_generator.pubsFromBib",
        "documentation": {}
    },
    {
        "label": "html_escape_table",
        "kind": 5,
        "importPath": "markdown_generator.pubsFromBib",
        "description": "markdown_generator.pubsFromBib",
        "peekOfCode": "html_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\nfor pubsource in publist:\n    parser = bibtex.Parser()",
        "detail": "markdown_generator.pubsFromBib",
        "documentation": {}
    },
    {
        "label": "html_escape",
        "kind": 2,
        "importPath": "markdown_generator.talks",
        "description": "markdown_generator.talks",
        "peekOfCode": "def html_escape(text):\n    if type(text) is str:\n        return \"\".join(html_escape_table.get(c,c) for c in text)\n    else:\n        return \"False\"\n# ## Creating the markdown files\n# \n# This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page.\n# In[5]:\nloc_dict = {}",
        "detail": "markdown_generator.talks",
        "documentation": {}
    },
    {
        "label": "talks",
        "kind": 5,
        "importPath": "markdown_generator.talks",
        "description": "markdown_generator.talks",
        "peekOfCode": "talks = pd.read_csv(\"talks.tsv\", sep=\"\\t\", header=0)\ntalks\n# ## Escape special characters\n# \n# YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely.\n# In[4]:\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"",
        "detail": "markdown_generator.talks",
        "documentation": {}
    },
    {
        "label": "html_escape_table",
        "kind": 5,
        "importPath": "markdown_generator.talks",
        "description": "markdown_generator.talks",
        "peekOfCode": "html_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\ndef html_escape(text):\n    if type(text) is str:\n        return \"\".join(html_escape_table.get(c,c) for c in text)\n    else:\n        return \"False\"",
        "detail": "markdown_generator.talks",
        "documentation": {}
    },
    {
        "label": "loc_dict",
        "kind": 5,
        "importPath": "markdown_generator.talks",
        "description": "markdown_generator.talks",
        "peekOfCode": "loc_dict = {}\nfor row, item in talks.iterrows():\n    md_filename = str(item.date) + \"-\" + item.url_slug + \".md\"\n    html_filename = str(item.date) + \"-\" + item.url_slug \n    year = item.date[:4]\n    md = \"---\\ntitle: \\\"\"   + item.title + '\"\\n'\n    md += \"collection: talks\" + \"\\n\"\n    if len(str(item.type)) > 3:\n        md += 'type: \"' + item.type + '\"\\n'\n    else:",
        "detail": "markdown_generator.talks",
        "documentation": {}
    },
    {
        "label": "DateTimeEncoder",
        "kind": 6,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "class DateTimeEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, (datetime, date)):\n            return obj.isoformat()\n        return super().default(obj)\ndef parse_markdown_cv(md_file):\n    \"\"\"Parse the markdown CV file and extract sections.\"\"\"\n    with open(md_file, 'r', encoding='utf-8') as file:\n        content = file.read()\n    # Remove YAML front matter",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_markdown_cv",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_markdown_cv(md_file):\n    \"\"\"Parse the markdown CV file and extract sections.\"\"\"\n    with open(md_file, 'r', encoding='utf-8') as file:\n        content = file.read()\n    # Remove YAML front matter\n    content = re.sub(r'^---.*?---\\s*', '', content, flags=re.DOTALL)\n    # Extract sections\n    sections = {}\n    current_section = None\n    section_content = []",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_config(config_file):\n    \"\"\"Parse the Jekyll _config.yml file for additional information.\"\"\"\n    if not os.path.exists(config_file):\n        return {}\n    with open(config_file, 'r', encoding='utf-8') as file:\n        config = yaml.safe_load(file)\n    return config\ndef extract_author_info(config):\n    \"\"\"Extract author information from the config file.\"\"\"\n    author_info = {",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "extract_author_info",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def extract_author_info(config):\n    \"\"\"Extract author information from the config file.\"\"\"\n    author_info = {\n        \"name\": config.get('name', ''),\n        \"email\": \"\",\n        \"phone\": \"\",\n        \"website\": config.get('url', ''),\n        \"summary\": \"\",\n        \"location\": {\n            \"address\": \"\",",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_education",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_education(education_text):\n    \"\"\"Parse education section from markdown.\"\"\"\n    education_entries = []\n    # Extract education entries\n    entries = re.findall(r'\\* (.*?)(?=\\n\\*|\\Z)', education_text, re.DOTALL)\n    for entry in entries:\n        # Parse degree, institution, and year\n        match = re.match(r'([^,]+), ([^,]+), (\\d{4})(.*)', entry.strip())\n        if match:\n            degree, institution, year, additional = match.groups()",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_work_experience",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_work_experience(work_text):\n    \"\"\"Parse work experience section from markdown.\"\"\"\n    work_entries = []\n    # Extract work entries\n    entries = re.findall(r'\\* (.*?)(?=\\n\\*|\\Z)', work_text, re.DOTALL)\n    for entry in entries:\n        lines = entry.strip().split('\\n')\n        if not lines:\n            continue\n        # Parse position and company",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_skills",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_skills(skills_text):\n    \"\"\"Parse skills section from markdown.\"\"\"\n    skills_entries = []\n    # Extract skill categories\n    categories = re.findall(r'(?:^|\\n)(\\w+.*?):\\s*(.*?)(?=\\n\\w+.*?:|\\Z)', skills_text, re.DOTALL)\n    for category, skills in categories:\n        # Extract individual skills\n        skill_list = [s.strip() for s in re.split(r',|\\n', skills) if s.strip()]\n        skills_entries.append({\n            \"name\": category.strip(),",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_publications",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_publications(pub_dir):\n    \"\"\"Parse publications from the _publications directory.\"\"\"\n    publications = []\n    if not os.path.exists(pub_dir):\n        return publications\n    for pub_file in sorted(glob.glob(os.path.join(pub_dir, \"*.md\"))):\n        with open(pub_file, 'r', encoding='utf-8') as file:\n            content = file.read()\n        # Extract front matter\n        front_matter_match = re.match(r'^---\\s*(.*?)\\s*---', content, re.DOTALL)",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_talks",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_talks(talks_dir):\n    \"\"\"Parse talks from the _talks directory.\"\"\"\n    talks = []\n    if not os.path.exists(talks_dir):\n        return talks\n    for talk_file in sorted(glob.glob(os.path.join(talks_dir, \"*.md\"))):\n        with open(talk_file, 'r', encoding='utf-8') as file:\n            content = file.read()\n        # Extract front matter\n        front_matter_match = re.match(r'^---\\s*(.*?)\\s*---', content, re.DOTALL)",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_teaching",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_teaching(teaching_dir):\n    \"\"\"Parse teaching from the _teaching directory.\"\"\"\n    teaching = []\n    if not os.path.exists(teaching_dir):\n        return teaching\n    for teaching_file in sorted(glob.glob(os.path.join(teaching_dir, \"*.md\"))):\n        with open(teaching_file, 'r', encoding='utf-8') as file:\n            content = file.read()\n        # Extract front matter\n        front_matter_match = re.match(r'^---\\s*(.*?)\\s*---', content, re.DOTALL)",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "parse_portfolio",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def parse_portfolio(portfolio_dir):\n    \"\"\"Parse portfolio items from the _portfolio directory.\"\"\"\n    portfolio = []\n    if not os.path.exists(portfolio_dir):\n        return portfolio\n    for portfolio_file in sorted(glob.glob(os.path.join(portfolio_dir, \"*.md\"))):\n        with open(portfolio_file, 'r', encoding='utf-8') as file:\n            content = file.read()\n        # Extract front matter\n        front_matter_match = re.match(r'^---\\s*(.*?)\\s*---', content, re.DOTALL)",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "create_cv_json",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def create_cv_json(md_file, config_file, repo_root, output_file):\n    \"\"\"Create a JSON CV from markdown and other repository data.\"\"\"\n    # Parse the markdown CV\n    sections = parse_markdown_cv(md_file)\n    # Parse config file\n    config = parse_config(config_file)\n    # Extract author information\n    author_info = extract_author_info(config)\n    # Create the JSON structure\n    cv_json = {",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scripts.cv_markdown_to_json",
        "description": "scripts.cv_markdown_to_json",
        "peekOfCode": "def main():\n    \"\"\"Main function to parse arguments and run the conversion.\"\"\"\n    parser = argparse.ArgumentParser(description='Convert markdown CV to JSON format')\n    parser.add_argument('--input', '-i', required=True, help='Input markdown CV file')\n    parser.add_argument('--output', '-o', required=True, help='Output JSON file')\n    parser.add_argument('--config', '-c', help='Jekyll _config.yml file')\n    args = parser.parse_args()\n    # Get repository root (parent directory of the input file's directory)\n    repo_root = str(Path(args.input).parent.parent)\n    create_cv_json(args.input, args.config, repo_root, args.output)",
        "detail": "scripts.cv_markdown_to_json",
        "documentation": {}
    },
    {
        "label": "TIMEOUT",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "TIMEOUT = 5\n# Collect the Markdown files\ng = glob.glob(\"_talks/*.md\")\n# Prepare to geolocate\ngeocoder = Nominatim(user_agent=\"academicpages.github.io\")\nlocation_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n# Perform geolocation",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "g",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "g = glob.glob(\"_talks/*.md\")\n# Prepare to geolocate\ngeocoder = Nominatim(user_agent=\"academicpages.github.io\")\nlocation_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "geocoder",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "geocoder = Nominatim(user_agent=\"academicpages.github.io\")\nlocation_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "location_dict",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "location_dict = {}\nlocation = \"\"\npermalink = \"\"\ntitle = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()\n    # Press on if the location is not present",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "location",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "location = \"\"\npermalink = \"\"\ntitle = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()\n    # Press on if the location is not present\n    if 'location' not in data:",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "permalink",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "permalink = \"\"\ntitle = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()\n    # Press on if the location is not present\n    if 'location' not in data:\n        continue",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "title",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "title = \"\"\n# Perform geolocation\nfor file in g:\n    # Read the file\n    data = frontmatter.load(file)\n    data = data.to_dict()\n    # Press on if the location is not present\n    if 'location' not in data:\n        continue\n    # Prepare the description",
        "detail": "talkmap",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "talkmap",
        "description": "talkmap",
        "peekOfCode": "m = getorg.orgmap.create_map_obj()\ngetorg.orgmap.output_html_cluster_map(location_dict, folder_name=\"talkmap\", hashed_usernames=False)",
        "detail": "talkmap",
        "documentation": {}
    }
]